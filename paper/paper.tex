% Template for ICASSP-2010 paper; to be used with:
%          mlspconf.sty  - ICASSP/ICIP LaTeX style file adapted for MLSP, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{amsmath,graphicx,mlspconf}
\usepackage{amsfonts}

%Select one of the four copyright notices below. Only required for the camera paper submission

%For papers in which all authors are employed by the US government, the copyright notice is:
\copyrightnotice{U.S.\ Government work not protected by U.S.\ copyright}

%For papers in which all authors are employed by a Crown government (UK, Canada, and Australia), the copyright notice is:
\copyrightnotice{978-1-5090-6341-3/17/\$31.00 {\copyright}2017 Crown}

%For papers in which all authors are employed by the European Union, the copyright notice is:
\copyrightnotice{978-1-5090-6341-3/17/\$31.00 {\copyright}2017 European Union}

%For all other papers the copyright notice is:
\copyrightnotice{978-1-5090-6341-3/17/\$31.00 {\copyright}2017 IEEE}

\toappear{2017 IEEE International Workshop on Machine Learning for Signal Processing, Sept.\ 25--28, 2017, Tokyo, Japan}


% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{A Neural Network Alternative to Convolutive Audio Models for Source Separation}
%
% Single address.
% ---------------
\name{Author(s) Name(s) omitted for double blind review\thanks{Thanks to XYZ agency for funding.}}
\address{Author Affiliation(s) omitted for double blind review}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%   Email A-B
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D
%   Email C-D}
%
\begin{document}
%\ninept
%

\maketitle
%
\begin{abstract}
\end{abstract}
%
\begin{keywords}
Auto-encoders, source separation, deep learning.
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Recently, several neural network architectures have been proposed to develop algorithms for supervised source separation and speech enhancement. (Chandna monaural, Grais Single channel, Park Fully convolutional, Venkataramani end-to-end source). Currently, these networks are trained to learn discriminative audio models extensively. In other words, the spectrogram of the mixture is given as an input to the network. The goal of the network then, is to learn suitable time-frequency masks that separate the input spectrogram into the source and the interference componenents. Thus, the networks learn a basis decomposition that often works only for a specific source-interference pair, i.e., these models are not transferable. If the interfering signal changes, these networks have to be re-trained to learn suitable models to separate the new interfering signal in the mixture from the source. \\

A popular technique to learn transferable audio models for supervised source separation is the use of Nnon-negative matrix factorization~(NMF). Non-negative matrix factorization~(NMF) matrix of non-negative elements~$\mathbf{X}\in \mathbb{R}_{M\times N}^{\geq0}$ as a product of the basis matrix~$\mathbf{W}$ and the activation matrix~$\mathbf{H}$. The notation $\mathbb{R}_{M \times N}^{\geq0}$ represents the set of matrices of non-negative elements of size $M \times N$. In this factorization, the basis matrix $\mathbf{W} \in \mathbb{R}_{M \times r}^{\geq{0}}$, the activation matrix $\mathbf{H} \in \mathbb{R}_{r \times N}^{\geq{0}}$ and $r$ represents the rank of the decomposition. In the case of audio signals, we apply such a factorization on audio spectrograms. In this setting, the columns of $\mathbf{W}$ begin to act as representative basis vectors for the source. The rows of $\mathbf{H}$ indicate the activity of these basis vectors in time. As shown in~~\cite{}(ICASSP Paper reference...), the notion of non-negative audio modeling can be easily generalized by interpreting NMF as a neural network. We can interpret NMF as a non-negative auto-encoder in the following manner,
\begin{align}
    \text{$1^{\text{st}}$ layer:~}\mathbf{H} &= g(\mathbf{W^{\ddagger}} \cdot \mathbf{X}) \\
    \text{$2^{\text{nd}}$ layer:~}\mathbf{X} &= g(\mathbf{W} \cdot \mathbf{H})
    \label{eq:nmfae}
\end{align}

Here, $\mathbf{X}$ represents the input spectrogram, $\mathbf{W^{\ddagger}}$ represents a form of pseudo-inverse of $\mathbf{W}$ and $g(.):\mathbf{R}\rightarrow \mathbf{R}^{\geq0}$ is an element-wise function that maps a real number to the space of positive real numbers. As before, the columns of $\mathbf{W}$ act as representative basis vectors and the corresponding rows of $\mathbf{H}$ indicate their respective activations. Although non-negativity of the models is not explicitly guaranteed in this formulation, applying a suitable sparsity constraint allows the network to learn suitable non-negative models. Additionally, this interpretation enables a pathway to propose multi-layer extensions by exploiting the wealth of available neural net architectures that could potentially lead to superior separation performance. \\

Spectrograms of speech and audio signals incorporate temporal dependencies that span multiple time frames. However, NMF and its neural network equivalent are unable to explicitly utilize these cross-frame patterns available in a spectrogram. To alleviate this drawback, Smaragdis~\cite{}(Paris convolutive speech bases) proposed a convolutive version to NMF that allows spectro-temporal patterns as representative basis elements. In this paper, we develop a neural network alternative to such convolutive audio models for supervised source separation. In doing so, we solve two fundamental problems associated with this task. The first step is to develop a suitable neural network architecture to learn convolutive audio models in an adaptive manner. Utilizing the models to separate a source from a given mixture forms the second step. The remainder of the paper is organized as follows. In section~\ref{sec:}, we develop an auto-encoder that can act as an equivalent to conv-NMF audio models. Section~\ref{sec:} proposes a novel approach to utilize these models for supervised source separation. We evaluate these models in terms of their separation performance in section~\ref{sec:} and conclude in section~\ref{cite:}. \\

\section{Non-negative Convolutiional Auto-encoders}
\subsection{Network Architecture}
\label{sec:conv-nmf}
The convolutive NMF model~\cite{}(Smaragdis convolutive speech bases) approximates a non-negative matrix~$\mathbf{X}\in \mathbb{R}_{M \times N}^{\geq0}$ as,
\begin{equation}
    \mathbf{X}(f,t) \approx \sum_{i=1}^{r} \sum_{k=0}^{T-1} \mathbf{W}_{i}(k,f)\cdot\mathbf{H}(i,t-k)
\end{equation}
Here, $\mathbf{W}_{i} \in \mathbb{R}_{M \times T}^{\geq0}$ acts as the $i^{\text{th}}$ basis matrix and $\mathbf{H} \in \mathbb{R}_{r \times N}^{\geq0}$ contains the corresponding weights. The notation $\mathbf{X}(i,j)$ represents the element of $\mathbf{X}$ indexed by the $i^{\text{th}}$ row and the $j^{\text{th}}$ column. Thus, we can interpret this operation as a two-step convolutional auto-encoder as follows,

\begin{align}
    \text{$1^{\text{st}}$ layer:~}\mathbf{H}(i,t) &= \sum_{i=1}^{r} \sum_{j=0}^{M-1} \sum_{k=0}^{T-1} \mathbf{W}_{i}^{\ddagger}(j,k)\mathbf{X}(j,t-k) \\
    \text{$2^{\text{nd}}$ layer:~}\hat{\mathbf{X}}(f,t) &= \sum_{i=1}^{r} \sum_{k=0}^{T-1} \mathbf{W}_{i}(k,f)\cdot\mathbf{H}(i,t-k)
    \label{eq:nmfcnncnn}
\end{align}
subject to non-negativity of $\mathbf{W}_{i}$ and $\mathbf{H}\geq0$. Here, we assume that the convolutional filters~$\mathbf{W},~\mathbf{W}^{\ddagger}$ have a size of $M\times T$ where, $T$ represents the depth of the convolution. In this representation, $\mathbf{W}_{i}$ and $\mathbf{H}$ correspond to the $i^{\text{th}}$ basis matrix and the activation matrix respectively. The filters of the first convolutional layer act as inverse filters in defining the auto-encoder. In the remainder of this section, we will refer to the first convolutional layer as the encoder that estimates a code from the input representation. The second convolutional layer generates an approximation of the input from the code and will be referred to as the decoder. We can simplify the non-negativity constraints by incorporating a non-linearity into the definitions of the encoder and the decoder. Thus,

\begin{align}
    \text{$1^{\text{st}}$ layer:~}\mathbf{H}(i,t) &= g\left(\sum_{i=1}^{r} \sum_{j=0}^{M-1} \sum_{k=0}^{T-1} \mathbf{W}_{i}^{\ddagger}(j,k)\mathbf{X}(j,t-k)\right) \\
    \text{$2^{\text{nd}}$ layer:~}\hat{\mathbf{X}}(f,t) &= g\left(\sum_{i=1}^{r} \sum_{k=0}^{T-1} \mathbf{W}_{i}(k,f)\cdot\mathbf{H}(i,t-k)\right)
    \label{eq:nmfcnncnn}
\end{align}
Here, the non-linearity $g(.):\mathbb{R}\rightarrow \mathbb{R^{\geq0}}$ applies an element-wise non-negativity constraint and ensures that the activation matrix and the reconstruction are non-negative. We now note a couple of key points about the convolutional auto-encoder. (i) The output of the encoder gives the latent representation of the decomposition. (ii) The weights of the decoder act as the basis vectors of the decomposition. (iii) We do not explicitly apply non-negativity constraints on the weights. Thus, the basis matrices (decoder filters) can assume negative values. 

To train the auto-encoder and learn the basis and activation matrices, in the remainder of the paper, we apply the following approach. We minimize the KL-divergence between the input spectrogram~$\mathbf{X}$ and its reconstruction~$\hat{\mathbf{X}}$ given by,
\begin{equation*}
    D\left(\mathbf{X},\hat{\mathbf{X}}\right) = \sum_{i,j} \mathbf{X}(i,j)\cdot \text{log}\frac{\mathbf{X}(i,j)}{\hat{\mathbf{X}}(i,j)} - \mathbf{X}(i,j) + \hat{\mathbf{X}}(i,j)
\end{equation*}
The network is trained by applying a batch gradient descent training procedure with a batchsize of (...) and the parameters updated using the RMSProp algorithm (**RMSProp reference**), with a learning rate and momentum of (..) and (..) respectively. 

\subsection{Practical Considerations}
\label{subsec:practical}
Having developed the convolutional auto-encoder equivalent to convolutive NMF, we can now begin to understand the nature of the basis and activation matrices learned by the network. To do so, we train the convolutive auto-encoder defined by~(\ref{eq:nmfcnncnn}) on a simple toy example as shown in figure~\ref{}(**Figure reference**)

% The approximation $\hat X$ for a given spectrogram $X$ is computed as follows: 

% \begin{align}
% 	\hat H(k,t) =& \sigma_1\left ( \sum_{f,t'} X(f,t-t') F_e(f,t',k) \right ) \notag \\
% 	\hat X(f,t) =& \sigma_2 \left ( \sum_k \sum_{t'} \hat H(k,t-t') F_d(f, t',k) \right )
% \end{align}



\section{Extensions to infinite summation}
\label{sec:rnncnn}


% This is the same as CNN-CNN case, except the computation of the activations $H$. In the CNN encoder, each filter was of finite length. With RNN-CNN version, we are attempting to use an infinite length filter. The computation of $\hat H$ is as follows: 

% \begin{align}
% 	Z(:,k,t) =& \sigma( W Z(:,k,t-1) + U X(:,t)) \notag \\
% 	\hat H(k,t) =& \sum_f Z(f,k,t)
% \end{align}

% Give a toy example which shows what this model can do that CNN-CNN can not. 

% \section{Multi-layer Extensions}
% \label{sec:multlayer}


\section{Supervised Source Separation}
\label{sec:ss}
Describe the separation procedure.

\section{Experiments}
\label{sec:experiments}
Try each model in a given $K$ range in speech-speech source separation task.

\subsection{Experimental setup}
\label{subsec:setup}

\subsection{Results and Discussion}
\label{subsec:results}

\section{Conclusion}
\label{sec:conclusion}


% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs.bib}

\end{document}
