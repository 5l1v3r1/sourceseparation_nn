% Template for ICASSP-2010 paper; to be used with:
%          mlspconf.sty  - ICASSP/ICIP LaTeX style file adapted for MLSP, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{amsmath,graphicx,mlspconf}
\usepackage{amsfonts}

%Select one of the four copyright notices below. Only required for the camera paper submission

%For papers in which all authors are employed by the US government, the copyright notice is:
\copyrightnotice{U.S.\ Government work not protected by U.S.\ copyright}

%For papers in which all authors are employed by a Crown government (UK, Canada, and Australia), the copyright notice is:
\copyrightnotice{978-1-5090-6341-3/17/\$31.00 {\copyright}2017 Crown}

%For papers in which all authors are employed by the European Union, the copyright notice is:
\copyrightnotice{978-1-5090-6341-3/17/\$31.00 {\copyright}2017 European Union}

%For all other papers the copyright notice is:
\copyrightnotice{978-1-5090-6341-3/17/\$31.00 {\copyright}2017 IEEE}

\toappear{2017 IEEE International Workshop on Machine Learning for Signal Processing, Sept.\ 25--28, 2017, Tokyo, Japan}


% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{A Neural Network Alternative to Convolutive Audio Models for Source Separation}
%
% Single address.
% ---------------
\name{Author(s) Name(s) omitted for double blind review\thanks{Thanks to XYZ agency for funding.}}
\address{Author Affiliation(s) omitted for double blind review}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%   Email A-B
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D
%   Email C-D}
%
\begin{document}
%\ninept
%

\maketitle
%
\begin{abstract}
\end{abstract}
%
\begin{keywords}
Auto-encoders, source separation, deep learning.
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Several neural network architectures have been proposed to develop algorithms for supervised source separation and speech enhancement. (Chandna monaural, Grais Single channel, Park Fully convolutional, Venkataramani end-to-end source). Currently, these networks are trained to learn discriminative audio models extensively. In other words, the spectrogram of the mixture is given as an input to the network. The goal of the network then, is to learn suitable time-frequency masks that separate the input spectrogram into the source and the interference componenents. Thus, the networks learn a basis decomposition that often works only for a specific source-interference pair, i.e., these models are not transferable. If the interfering signal changes, these networks have to be re-trained to learn suitable models to separate the new interfering signal in the mixture from the source. \\

A popular technique to learn transferable audio models for supervised source separation is the use of Nnon-negative matrix factorization~(NMF). Non-negative matrix factorization~(NMF) matrix of non-negative elements~$\mathbf{X}\in \mathbb{R}_{M\times N}^{\geq0}$ as a product of the basis matrix~$\mathbf{W}$ and the activation matrix~$\mathbf{H}$. The notation $\mathbb{R}_{M \times N}^{\geq0}$ represents the set of matrices of non-negative elements of size $M \times N$. In this factorization, the basis matrix $\mathbf{W} \in \mathbb{R}_{M \times r}^{\geq{0}}$, the activation matrix $\mathbf{H} \in \mathbb{R}_{r \times N}^{\geq{0}}$ and $r$ represents the rank of the decomposition. In the case of audio signals, we apply such a factorization on audio spectrograms. In this setting, the columns of $\mathbf{W}$ begin to act as representative basis vectors for the source. The rows of $\mathbf{H}$ indicate the activity of these basis vectors in time. As shown in~~\cite{}(ICASSP Paper reference...), the notion of non-negative audio modeling can be easily generalized by interpreting NMF as a neural network. We can interpret NMF as a non-negative auto-encoder in the following manner,
\begin{align}
    \text{$1^{\text{st}}$ layer:~}\mathbf{H} &= g(\mathbf{W^{\ddagger}} \cdot \mathbf{X}) \\
    \text{$2^{\text{nd}}$ layer:~}\mathbf{X} &= g(\mathbf{W} \cdot \mathbf{H})
    \label{eq:nmfae}
\end{align}

Here, $\mathbf{X}$ represents the input spectrogram, $\mathbf{W^{\ddagger}}$ represents a form of pseudo-inverse of $\mathbf{W}$ and $g(.):\mathbf{R}\rightarrow \mathbf{R}^{\geq0}$ is an element-wise function that maps a real number to the space of positive real numbers. As before, the columns of $\mathbf{W}$ act as representative basis vectors and the corresponding rows of $\mathbf{H}$ indicate their respective activations. Although non-negativity of the models is not explicitly guaranteed in this formulation, applying a suitable sparsity constraint allows the network to learn suitable non-negative models. Additionally, this interpretation enables a pathway to propose multi-layer extensions by exploiting the wealth of available neural net architectures that could potentially lead to superior separation performance. \\

Spectrograms of speech and audio signals incorporate temporal dependencies that span multiple time frames. However, NMF and its neural network equivalent are unable to explicitly utilize these cross-frame patterns available in a spectrogram. To alleviate this drawback, Smaragdis~\cite{}(Paris convolutive speech bases) proposed a convolutive version to NMF that allows spectro-temporal patterns as representative basis elements. In this paper, we develop a neural network alternative to such convolutive audio models for supervised source separation. In doing so, we solve two fundamental problems associated with this task. The first step is to develop a suitable neural network architecture to learn convolutive audio models in an adaptive manner. Utilizing the models to separate a source from a given mixture forms the second step. The remainder of the paper is organized as follows. In section~\ref{sec:}, we develop an auto-encoder that can act as an equivalent to conv-NMF audio models. Section~\ref{sec:} proposes a novel approach to utilize these models for supervised source separation. We evaluate these models in terms of their separation performance in section~\ref{sec:} and conclude in section~\ref{cite:}. \\

\section{Non-negative Convolutiional Auto-encoders}
\subsection{Network Architecture}
\label{sec:conv-nmf}
The convolutive NMF model~\cite{}(Smaragdis convolutive speech bases) approximates a non-negative matrix~$\mathbf{X}\in \mathbb{R}_{M \times N}^{\geq0}$ as,
\begin{equation}
    \mathbf{X}(f,t) \approx \sum_{i=1}^{r} \sum_{k=0}^{T-1} \mathbf{W}_{i}(k,f)\cdot\mathbf{H}(i,t-k)
\end{equation}
Here, $\mathbf{W}_{i} \in \mathbb{R}_{M \times T}^{\geq0}$ acts as the $i^{\text{th}}$ basis matrix and $\mathbf{H} \in \mathbb{R}_{r \times N}^{\geq0}$ contains the corresponding weights. The notation $\mathbf{X}(i,j)$ represents the element of $\mathbf{X}$ indexed by the $i^{\text{th}}$ row and the $j^{\text{th}}$ column. Thus, we can interpret this operation as a two-step convolutional auto-encoder as follows,

\begin{align}
    \text{$1^{\text{st}}$ layer:~}\mathbf{H}(i,t) &= \sum_{i=1}^{r} \sum_{j=0}^{M-1} \sum_{k=0}^{T-1} \mathbf{W}_{i}^{\ddagger}(j,k)\mathbf{X}(j,t-k) \\
    \text{$2^{\text{nd}}$ layer:~}\hat{\mathbf{X}}(f,t) &= \sum_{i=1}^{r} \sum_{k=0}^{T-1} \mathbf{W}_{i}(k,f)\cdot\mathbf{H}(i,t-k)
    \label{eq:nmfcnncnn}
\end{align}
subject to non-negativity of $\mathbf{W}_{i}$ and $\mathbf{H}$. Here, we assume that the convolutional filters~$\mathbf{W},~\mathbf{W}^{\ddagger}$ have a size of $M\times T$ where, $T$ represents the depth of the convolution. In this representation, $\mathbf{W}_{i}$ and $\mathbf{H}$ correspond to the $i^{\text{th}}$ basis matrix and the activation matrix respectively. The filters of the first convolutional layer act as inverse filters in defining the auto-encoder. In the remainder of this section, we will refer to the first convolutional layer as the encoder that estimates a code from the input representation. The second convolutional layer generates an approximation of the input from the code and will be referred to as the decoder. We can simplify the non-negativity constraints by incorporating a non-linearity into the definitions of the encoder and the decoder. Thus,

\begin{align}
    \text{$1^{\text{st}}$ layer:~}\mathbf{H}(i,t) &= g\left(\sum_{i=1}^{r} \sum_{j=0}^{M-1} \sum_{k=0}^{T-1} \mathbf{W}_{i}^{\ddagger}(j,k)\mathbf{X}(j,t-k)\right) \\
    \text{$2^{\text{nd}}$ layer:~}\hat{\mathbf{X}}(f,t) &= g\left(\sum_{i=1}^{r} \sum_{k=0}^{T-1} \mathbf{W}_{i}(k,f)\cdot\mathbf{H}(i,t-k)\right)
    \label{eq:nmfcnncnn}
\end{align}
Here, the non-linearity $g(.):\mathbb{R}\rightarrow \mathbb{R}^{\geq0}$ applies an element-wise non-negativity constraint and ensures that the activation matrix and the reconstruction are non-negative. We now note a couple of key points about the convolutional auto-encoder. (i) The output of the encoder gives the latent representation of the decomposition. (ii) The weights of the decoder act as the basis vectors of the decomposition. (iii) We do not explicitly apply non-negativity constraints on the weights. Thus, the basis matrices (decoder filters) can assume negative values. 

To train the auto-encoder and learn the basis and activation matrices, in the remainder of the paper, we apply the following approach. We minimize the KL-divergence between the input spectrogram~$\mathbf{X}$ and its reconstruction~$\hat{\mathbf{X}}$ given by,
\begin{equation*}
    D\left(\mathbf{X},\hat{\mathbf{X}}\right) = \sum_{i,j} \mathbf{X}(i,j)\cdot \text{log}\frac{\mathbf{X}(i,j)}{\hat{\mathbf{X}}(i,j)} - \mathbf{X}(i,j) + \hat{\mathbf{X}}(i,j)
\end{equation*}
The network is trained by applying a batch gradient descent training procedure with a batchsize of (...) and the parameters updated using the RMSProp algorithm (**RMSProp reference**), with a learning rate and momentum of (..) and (..) respectively. 

\subsection{Practical Considerations}
\label{subsec:practical}
Having developed the convolutional auto-encoder equivalent to convolutive NMF, we can now begin to understand the nature of the basis and activation matrices learned by the network. To do so, we train the convolutive auto-encoder defined by~(\ref{eq:nmfcnncnn}) on a simple toy example as shown in figure~\ref{}(**Figure reference**).

% The approximation $\hat X$ for a given spectrogram $X$ is computed as follows: 

% \begin{align}
% 	\hat H(k,t) =& \sigma_1\left ( \sum_{f,t'} X(f,t-t') F_e(f,t',k) \right ) \notag \\
% 	\hat X(f,t) =& \sigma_2 \left ( \sum_k \sum_{t'} \hat H(k,t-t') F_d(f, t',k) \right )
% \end{align}



\section{Extensions to infinite summation}
\label{sec:rnncnn}


% This is the same as CNN-CNN case, except the computation of the activations $H$. In the CNN encoder, each filter was of finite length. With RNN-CNN version, we are attempting to use an infinite length filter. The computation of $\hat H$ is as follows: 

% \begin{align}
% 	Z(:,k,t) =& \sigma( W Z(:,k,t-1) + U X(:,t)) \notag \\
% 	\hat H(k,t) =& \sum_f Z(f,k,t)
% \end{align}

% Give a toy example which shows what this model can do that CNN-CNN can not. 

% \section{Multi-layer Extensions}
% \label{sec:multlayer}


\section{Supervised Source Separation}
\label{sec:ss}
The problem of supervised source separation is solved as a two-step procedure~\cite{}(**Paris supervised and semi-supervised**). The fist step of the procedure is to learn suitable models for a given source. We refer to this step as the training step. In the second step, we use these models to explain the contribution of the source in an unknown mixture. In section~\ref{sec:conv-nmf}, we have developed the auto-encoder architecture to learn suitable convolutive models for a given source. We now turn our attention to the problem of using the models for separating the source in an unknown mixture.

The previous approach to source separation~\cite{}(**ICASSP paper**) involves estimating the latent representation of the bases, which captures the contribution of the bases to each frame of the mixture spectrogram. This does not utilize the encoder component of the auto-encoder for the separation task. In this paper, we present a novel-separation scheme that utilizes the complete auto-encoder architecture for separation. We do so by using the following setup for separation. Given an input spectrogram $\mathbf{X}$, the auto-encoder produces an approximation that models the spectrogram in terms of its weights. We will denote to this approximation as,
\begin{equation}
    \hat{\mathbf{X}} = Ae(\mathbf{X}|\theta)
    \label{eq:separation_ae}
\end{equation}
Here, $\theta$ denotes the weights (parameters) of the auto-encoder. Thus, given the trained auto-encoders, i.e., given $\theta_{1}$ and $\theta_{2}$, the goal of separation is to identify suitable spectrograms $\mathbf{X}_{1}$ and $\mathbf{X}_{2}$ such that,
\begin{equation}
    \mathbf{X}_{m} = Ae(\mathbf{X}_{1}|\theta_{1}) + Ae(\mathbf{X}_{2}|\theta_{2}) 
    \label{eq:separation}
\end{equation}
Here, $\mathbf{X}_{m}$ represents the spectrogram of the mixture and $\mathbf{X}_{1}$,~$\mathbf{X}_{2}$ denote the separated source spectrograms. In other words, the separation procedure attempts to estimate the mixture spectrogram as the sum of spectrograms of the individual sources. This assumption of additivity of spectrograms is similar to the separation procedure used in~\cite{}(**NMF, Icassp**). However, we directly estimate the source spectrograms without estimating the latent representation. We train the network defined by~(\ref{eq:separation}) for an appropriate input instead of training for the weights of the network. As before, we minimize the KL divergence between mixture spectrogram $\mathbf{X}_{m}$ and its approximation~$\mathbf{X}_{1} + \mathbf{X}_{2}$. Conceptually, this problem is not different to training a neural network. The equivalence can be seen by applying a transposition to the auto-encoder definitions in (\ref{eq:nmfcnncnn}). This also allows a generalized separation procedure to be applied even when the underlying architectures of the auto-encoders are changed.

Having obtained the contributions of the sources (separated spectrograms), the next step is to transform these spectrograms back into the time domain. This is given as,
\begin{equation}
    x_{i}(t) = \text{STFT}^{-1}\left(\frac{\mathbf{X_{i}}}{\sum_{i}\mathbf{X}_{i}}\odot \mathbf{X}_{m} \odot e^{i\Phi_{m}}\right)~\text{for}~i\in\{1, 2\}
\end{equation}
Here $x_{i}(t)$ denotes the separated speech signal in time and $\Phi_{m}$ represents the phase of the mixture. Also, $\odot$ represents the element-wise multiplication operation and the division is also element-wise.

\section{Experiments}
\label{sec:experiments}
We now describe the experiments used to evaluate our auto-encoder based convolutive audio models. We do so by comparing the separation performance of these models to the neural network based models proposed in~\cite{}(**ICASSP paper**). For a uniform experimental setup, we apply a separation scheme described in~\ref{sec:ss} to both the models. We compare the performance in terms of median BSS\_eval metrics~\cite{}(**BSS eval paper**) viz., signal-to-distortion (SDR), signal-to-interference (SIR) and signal-to-artifact ratio (SAR) parameters. We also compare the median intelligibility scores in terms of the STOI index~\cite{}(**STOI paper**). To compare these models, we use the TIMIT corpus~\cite{} (**Timit reference**). To form these mixtures, we randomly select a pair of male-female speakers from the TIMIT corpus. Of the $10$ utterances available for each speaker, one utterance is randomly selected for each speaker. These two selected utterances are mixed at $0 dB$ to generate the testing mixture. For the evaluation, we generate $20$ such mixtures and compare the models for different parameter configurations. As the pre-processing step, we apply a $1024$ point short-time Fourier transform representation with a hop of $25\%$. The magnitude spectrogram is then given as an input to the network. 

We perform these evaluations at multiple parameter configurations with varying values for decomposition rank and filter size.

*** Talk about how you vary or select the values of K and T***

\subsection{Experimental setup}
\label{subsec:setup}

\subsection{Results and Discussion}
\label{subsec:results}

\section{Conclusion}
\label{sec:conclusion}


% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs.bib}

\end{document}
